\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{bm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

% 设置页边距
\geometry{
    left=2cm,
    right=2cm,
    top=2cm,
    bottom=2cm,
}

\begin{document}

\title{5004 Homework 2}
\author{RONG Shuo}
\date{\today}
\maketitle


\section*{Question 1:}
1. For each of the following functions \(f(x_1, x_2)\), find all critical points (i.e, all \(x_1, x_2\) such that \( \nabla f(x_1, x_2) = \bm{0}\)). \\

(a) \(f(x_1, x_2) = (4x_1^2 - x_2)^2\) \\
(b) \( f(x_1, x_2) = 2x_2^3 - 6x_2^2 + 3x_1^2x_2\) \\
(c) \(f(x_1, x_2) = (x_1 - 2x_2)^4 + 64x_1x_2\) \\
(d) \(f(x_1, x_2) = x_1^2 + 4x_1x_2 + x_2^2 + x_1 - x_2\) \\

\subsection*{Answer :}
(a)
\begin{align*}
    \frac{\partial f}{ \partial x_1} = 2(4x_1^2 - x_2)(8x_1) = 16x_1(4x_1^2 - x_2) \\
    \frac{\partial f}{ \partial x_2} = 2(4x_1^2 - x_2)(-1) = -2(4x_1^2 - x_2)
\end{align*}
set the gradient to 0:
\begin{align}
    16x_1(4x_1^2 - x_2) = 0 \\
    -2(4x_1^2 - x_2) = 0 
\end{align}
if \(x_1 = 0\), from (2) we can get that \(x_2 = 0\)
if \(x_1 \ne 0\), from equation (1), we know:
\begin{align*}
    4x_1^2 = x_2
\end{align*}
this satisfied \((x_1, x_2) = (0, 0)\)
Thus, we can conclude that the critical points are:
\begin{align*}
    (x_1, x_2) = (x_1, 4x_1^2) \forall x_1 \in \R.
\end{align*}


(b) 
\begin{align*}
    \frac{\partial f}{\partial x_1} = 6x_1x_2 \\
    \frac{\partial f}{\partial x_2} = 6x_2^2 - 12x_2 + 3x_1^2 
\end{align*}
set the gradient to 0:
\begin{align*}
    6x_1x_2 = 0 \\
    6x_2^2 - 12x_2 + 3x_1^2 = 0
\end{align*}
if \(x_1 = 0\), 
\begin{align*}
    6x_2^2 - 12x_2 = 0
    6x_2(x_2 - 2) = 0
\end{align*}
we can conclude that \((x_1 ,x_2) = (0, 0), \text{ or } (x_1 ,x_2) = (0, 2).\)
if \(x_2 = 0\), 
\begin{align*}
    3x_1^2 = 0
    x_1 = 0
\end{align*}
This gives \((x_1, x_2) = (0, 0)\) \\
In conclusion, the critical points is \((0, 0) \text{ and }(0, 2)\)

(c)
\begin{align*}
    \frac{}{}
\end{align*}




\section*{Question 2:}
2. Find the gradient of the following functions, where the space \(\R\) and \(\R^{n \times n}\) are equipped with the standard inner product. \\
(a) \(f(\bm{x}) = \frac{1}{2} \|\bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda\|\bm{x}\|_2^2\), where \(\bm{A} \in \R^{m \times n}\), \(b \in \R^m\), and (\(\lambda > 0\)) are given. \\
(b) \(f(\bm{X})\) = \(\bm{b}^T\bm{X}\bm{c}\), where \(\bm{X} \in \R^{n \times n}\) and \(\bm{b}, \bm{c} \in \R^n\) \\
(c) \(f(\bm{X})\) = \(\bm{b}\bm{X}^T\bm{X}c\), where \(\bm{X} \in \bm{R}^{n \times n}\) and \(\bm{b}, \bm{c} \in \R^n\) 
\subsection*{Answer :}

\section*{Question 3:}
3. Let \(\{\bm{x}_i, y_i\}_{i = 1}^N\) be given with \(\bm{x}_i \in \R\) and \(y_i \in \R\). Assume \(N < n\). Consider the ridge regression
\begin{align*}
    \text{minimize}_{\bm{a}\in\R^N} \sum_{i = 1}^N (\langle \bm{\alpha}, \bm{x}_i\rangle - y_i)^2 + \lambda\|\bm{a}\|_2^2,
\end{align*}
where \(\lambda \in \R\) is a regularization parameter, and we set the bias \(b = 0\) for simplicity. \\

(a) Prove that the solution must be in the form of \(\bm{a} = \sum_{i = 1}^N c_i \bm{x}_i\) for some \(\bm{c} = [c_1, c_2, \cdots, c_N]^T \in \R^N\). \\
(\textit{hint: similar to the proof of the representer theorem.})

(b) Re-express the minimization in terms of \(c \in \R^N\), which has fewer unknowns than the original formulation as \(N < n\).


\subsection*{Answer :}
\section*{Question 4:}
4. Let \(f(\bm{x}) = \bm{x}^T\bm{A}\bm{x} + 2\bm{b}^T\bm{x} + c\), where \(\bm{A} \in \R^{n \times n}\) is a symmetric positive semidefinite matrix, \(b \in \R^n\), and \(c \in \R\). \\
(a) Prove that \(\bm{x}\) is a global minimizer of \(f\) if and only if \(\bm{A}\bm{x} = -\bm{b}\). \\
(b) Prove that \(f\) is bounded below over \(\R^n\) if and only if \(\bm{b} \in \{\bm{A}\bm{y}: \bm{y} \in \R^n\}\). \\
\subsection*{Answer :}

\section*{Question 5:}
5. We consider the following optimization problem:
\begin{align}
    \text{minimize}_{\bm{x} \in \R^n} f(\bm{x}) = log\left( \sum_{i = 1}^m \text{exp}(\bm{a}_i^T \bm{x} + b_i)\right)
\end{align}

where \(\bm{a}_1, \cdots \bm{a}_m \in \R^n \text{ and } b_1, \cdots b_m \in \R\) are given. \\

(a) Find the gradient of \(f(\bm{x})\). \\
(b) If we use gradient descent to solve Problem (1), will it converge to the global minimizer? Please justify your answer.


\subsection*{Answer}
\end{document}