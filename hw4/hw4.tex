\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{bm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

% 设置页边距
\geometry{
    left=2cm,
    right=2cm,
    top=2cm,
    bottom=2cm,
}

\begin{document}

\title{5004 Homework 2}
\author{RONG Shuo}
\date{\today}
\maketitle


\section*{Question 1:}
1. For each of the following functions \(f(x_1, x_2)\), find all critical points (i.e, all \(x_1, x_2\) such that \( \nabla f(x_1, x_2) = \bm{0}\)). \\

(a) \(f(x_1, x_2) = (4x_1^2 - x_2)^2\) \\
(b) \( f(x_1, x_2) = 2x_2^3 - 6x_2^2 + 3x_1^2x_2\) \\
(c) \(f(x_1, x_2) = (x_1 - 2x_2)^4 + 64x_1x_2\) \\
(d) \(f(x_1, x_2) = x_1^2 + 4x_1x_2 + x_2^2 + x_1 - x_2\) \\

\subsection*{Answer :}
(a)
\begin{align*}
    \frac{\partial f}{ \partial x_1} &= 2(4x_1^2 - x_2)(8x_1) = 16x_1(4x_1^2 - x_2) \\
    \frac{\partial f}{ \partial x_2} &= 2(4x_1^2 - x_2)(-1) = -2(4x_1^2 - x_2)
\end{align*}
set the gradient to 0:
\begin{align}
    16x_1(4x_1^2 - x_2) &= 0 \\
    -2(4x_1^2 - x_2) &= 0 
\end{align}
if \(x_1 = 0\), from (2) we can get that \(x_2 = 0\) \\
if \(x_1 \ne 0\), from equation (1), we know:
\begin{align*}
    4x_1^2 = x_2
\end{align*}
this satisfied \((x_1, x_2) = (0, 0)\)
Thus, we can conclude that the critical points are:
\begin{align*}
    (x_1, x_2) &= (x_1, 4x_1^2) ,\forall x_1 \in \R.
\end{align*}


(b) 
\begin{align*}
    \frac{\partial f}{\partial x_1} &= 6x_1x_2 \\
    \frac{\partial f}{\partial x_2} &= 6x_2^2 - 12x_2 + 3x_1^2 
\end{align*}
set the gradient to 0:
\begin{align*}
    6x_1x_2 &= 0 \\
    6x_2^2 - 12x_2 + 3x_1^2 &= 0
\end{align*}
if \(x_1 = 0\), 
\begin{align*}
    6x_2^2 - 12x_2 &= 0 \\
    6x_2(x_2 - 2) &= 0
\end{align*}
we can conclude that \((x_1 ,x_2) = (0, 0), \text{ or } (x_1 ,x_2) = (0, 2).\) \\
if \(x_2 = 0\), 
\begin{align*}
    3x_1^2 &= 0 \\
    x_1 &= 0
\end{align*}
This gives \((x_1, x_2) = (0, 0)\) \\
In conclusion, the critical points is \((0, 0) \text{ and }(0, 2)\)

(c)
\begin{align*}
    \frac{f}{\partial x_1} &= 4(x_1 - 2x_2)^3 + 64x_2 \\
    \frac{f}{\partial x_2} &= -8(x_1 - 2x_2)^3 + 64x_1
\end{align*}
set the gradient to 0:
\begin{align*}
    4(x_1 - 2x_2)^3 + 64x_2 &= 0 \\
    \text{ i.e. } (x_1 - 2x_2)^2 &= -16x_2 \\
    -8(x_1 - 2x_2)^3 + 64x_1 &= 0 \\
    \text{ i.e. }(x_1 - 2x_2)^3 &= 8x_1
\end{align*}
\begin{align*} 
    (x_1 - 2x_2)^3 &= -16x_2 \\
    (x_1 - 2x_2)^3 &= 8x_1 \\
    -16x_2 &= 8x_1 \\
    -2x_2 &= x_1
\end{align*}
Substituting \(-2x_2 = x_1 \) to \((x_1 - 2x_2)^2 = -16x_2\), we can get:
\begin{align*}
    64x_2^3 + 16x_2 &= 0 \\
    x_2^2 &= \frac{1}{4} 
\end{align*}
Thus, the result is
\begin{align*}
    (x_1, x_2) &= (-1, \frac{1}{2}) \\
    (x_1, x_2) &= (1, -\frac{1}{2})
\end{align*}

(d)
\begin{align*}
    \frac{\partial f}{\partial x_1} &= 2x_1 + 4x_2 + 1 \\
    \frac{\partial f}{\partial x_2} &= 4x_1 + 2x_2 - 1
\end{align*}
set the gradient to 0:
\begin{align*}
    2x_1 + 4x_2 + 1 &= 0 \\
    4x_1 + 2x_2 - 1 &= 0
\end{align*}
\begin{align*}
    x_1 &= - \frac{1}{2} - 2x_2 
\end{align*}
Substituting this to second equation.
\begin{align*}
    4(- \frac{1}{2} - 2x_2) + 2x_2 - 1 &= 0 \\
    -2 - 8x_2 + 2x_2 - 1 &= 0 \\
    x_2 &= -\frac{1}{2} \\
    x_1 &= -\frac{1}{2} + 1 = \frac{1}{2}
\end{align*}
Thus, the critical point is \(\left( \frac{1}{2}, -\frac{1}{2} \right)\)





\section*{Question 2:}
2. Find the gradient of the following functions, where the space \(\R\) and \(\R^{n \times n}\) are equipped with the standard inner product. \\
(a) \(f(\bm{x}) = \frac{1}{2} \|\bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda\|\bm{x}\|_2^2\), where \(\bm{A} \in \R^{m \times n}\), \(b \in \R^m\), and (\(\lambda > 0\)) are given. \\
(b) \(f(\bm{X})\) = \(\bm{b}^T\bm{X}\bm{c}\), where \(\bm{X} \in \R^{n \times n}\) and \(\bm{b}, \bm{c} \in \R^n\) \\
(c) \(f(\bm{X})\) = \(\bm{b}\bm{X}^T\bm{X}c\), where \(\bm{X} \in \bm{R}^{n \times n}\) and \(\bm{b}, \bm{c} \in \R^n\) 
\subsection*{Answer :}
(a)
\begin{align*}
    f(\bm{y}) &= \frac{1}{2} \|\bm{A}\bm{y} - \bm{b}\|_2^2 + \lambda\|\bm{y}\|_2^2 \\
    f(\bm{y}) &= \frac{1}{2} \|\bm{A}\bm{y} - \bm{A}\bm{x} + \bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda\|\bm{y} - \bm{x} + \bm{x}\|_2^2 \\
    f(\bm{y}) &= \frac{1}{2} \|\bm{A}(\bm{y} - \bm{x}) + \bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda\|(\bm{y} - \bm{x}) + \bm{x}\|_2^2 \\
    f(\bm{y}) &= \frac{1}{2} \|\bm{A}(\bm{y} - \bm{x})\|_2^2 + \langle \bm{A} (\bm{y} - \bm{x}), \bm{A}\bm{x} -\bm{b} \rangle+ \frac{1}{2}\|\bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda\|\bm{y} - \bm{x}\|_2^2 + \lambda\langle \bm{y} - \bm{x}, \bm{x}\rangle + \lambda\|\bm{x}\|_2^2 \\
    f(\bm{y}) &= \frac{1}{2} \|\bm{A}(\bm{y} - \bm{x})\|_2^2 + \langle(\bm{y} - \bm{x}), \bm{A}^T(\bm{A}\bm{x} -\bm{b}) \rangle+ \frac{1}{2}\|\bm{A}\bm{x} - \bm{b}\|_2^2 + \lambda\|\bm{y} - \bm{x}\|_2^2 + \lambda\langle \bm{y} - \bm{x}, \bm{x}\rangle + \lambda\|\bm{x}\|_2^2 \\
    f(\bm{y}) &= \frac{1}{2} \|\bm{A}(\bm{y} - \bm{x})\|_2^2 + \langle(\bm{y} - \bm{x}), \bm{A}^T(\bm{A}\bm{x} -\bm{b}) \rangle+ f(\bm{x}) + \lambda\|\bm{y} - \bm{x}\|_2^2 + \lambda\langle \bm{y} - \bm{x}, \bm{x}\rangle \\
\end{align*}
\begin{align*}
    &\lim_{\|\bm{y} - \bm{x}\|_2 \to 0} \frac{ |f(\bm{y}) - (f(\bm{x}) + \langle \bm{A}^T(\bm{A}\bm{x} - \bm{b}), \bm{y} - \bm{x}\rangle + \lambda \langle \bm{y} - \bm{x}, \bm{x} \rangle)| }{\|\bm{y} - \bm{x}\|_2} \\
    &= \lim_{\|\bm{y} - \bm{x}\|_2 \to 0} \frac{\frac{1}{2} \|\bm{A}(\bm{y} - \bm{x})\|_2^2 + \lambda \| \bm{y} - \bm{x}\|_2^2}{\|\bm{y} - \bm{x}\|_2} \\
    &\leq \lim_{\|\bm{y} - \bm{x}\|_2 \to 0} \frac{\frac{1}{2} \|\bm{A}\|_2^2\|\bm{y} - \bm{x}\|_2^2 + \lambda \| \bm{y} - \bm{x}\|_2^2}{\|\bm{y} - \bm{x}\|_2} \\
    &= \lim_{\|\bm{y} - \bm{x}\|_2 \to 0} \frac{1}{2} \|\bm{A}\|_2^2\|\bm{y} - \bm{x}\|_2 + \lambda \| \bm{y} - \bm{x}\|_2 \\
    &= 0
\end{align*}
In conclusion, the gradient for(a) is \(\nabla f(\bm{x}) = \bm{A}^T(\bm{A}\bm{x} - \bm{b}) + 2\lambda \bm{x}\) \\

(b) 
We know the definition of the \((\nabla_{\bm{X}} f(\bm{X}))_{ij} = \frac{\partial f}{\partial \bm{X}_{ij}} \) \\
And we know that \(f(\bm{X})\)
\begin{align*}
    f(\bm{X}) &= \sum_{i=1}^{n}\sum_{j=1}^{n} b_iX_{ij}c_j \\
    \frac{\partial f}{\partial X_{ij}} &= b_ic_j \\ 
    \nabla_{\bm{X}} f(\bm{X}) &= \begin{bmatrix}
        b_1c_1 & b_1c_2 & \cdots & b_1c_n \\
        b_2c_1 & b_2c_2 & \cdots & b_2c_n \\
        \vdots &\vdots & \vdots &\vdots \\
        b_nc_1 & b_nc_2 & \cdots & b_nc_n \\
    \end{bmatrix} \\
    \nabla_{\bm{X}} f(\bm{X}) &= \bm{b}\bm{c}^T
\end{align*} \\
In conclusion, the gradient of \(f(\bm{X})\) = \(\bm{b}^T\bm{X}\bm{c}\) is \(\nabla_{\bm{X}} f(\bm{X}) = \bm{b}\bm{c}^T\) \\

(c) We consider\\
\begin{align*}
    f(\bm{X}) &= \sum_{i=1}^{n}\sum_{j=1}^{n} b_iX_{ji}X_{ij}c_j \\
    \frac{\partial f}{\partial X_{ij}} &= b_iX_{ji}c_j + b_iX_{ji}c_j = 2b_iX_{ji}c_j    \\ 
    \nabla_{\bm{X}} f(\bm{X}) &= \begin{bmatrix}
        2b_1\bm{X}_{11}c_1 & 2b_1\bm{X}_{21}c_2 & \cdots & 2b_1\bm{X}_{n1}c_n \\
        2b_2\bm{X}_{12}c_1 & 2b_2\bm{X}_{22}c_2 & \cdots & 2b_2\bm{X}_{n2}c_n \\
        \vdots &\vdots & \vdots &\vdots \\
        2b_n\bm{X}_{1n}c_1 & 2b_n\bm{X}_{2n}c_2 & \cdots & 2b_n\bm{X}_{nn}c_n \\
    \end{bmatrix} \\
    \nabla_{\bm{X}} f(\bm{X}) &= 2\bm{b}\bm{X}\bm{c}^T
\end{align*}



\section*{Question 3:}
3. Let \(\{\bm{x}_i, y_i\}_{i = 1}^N\) be given with \(\bm{x}_i \in \R\) and \(y_i \in \R\). Assume \(N < n\). Consider the ridge regression \begin{align*}
    \text{minimize}_{\bm{a}\in\R^N} \sum_{i = 1}^N (\langle \bm{a}, \bm{x}_i\rangle - y_i)^2 + \lambda\|\bm{a}\|_2^2,
\end{align*}
where \(\lambda \in \R\) is a regularization parameter, and we set the bias \(b = 0\) for simplicity. \\

(a) Prove that the solution must be in the form of \(\bm{a} = \sum_{i = 1}^N c_i \bm{x}_i\) for some \(\bm{c} = [c_1, c_2, \cdots, c_N]^T \in \R^N\). \\
(\textit{hint: similar to the proof of the representer theorem.})

(b) Re-express the minimization in terms of \(c \in \R^N\), which has fewer unknowns than the original formulation as \(N < n\).


\subsection*{Answer :}
(a) We can denote \(\bm{a} = \bm{a}_s + \sum_{i=1}^N c_i \bm{x}_i\), where \(\bm{c} = [c_1, c_2, \cdots, c_N]^T \in \R^N\), and \(\langle \bm{a}_s, \bm{x}_i\rangle = 0\).
\begin{align*}
    \sum_{i = 1}^N (\langle \bm{a}, \bm{x}_i\rangle - y_i)^2 + \lambda\|\bm{a}\|_2^2 &= \sum_{i = 1}^N (\langle \bm{a}_s + \sum_{j=1}^N c_j \bm{x}_j, \bm{x}_i\rangle - y_i)^2 + \lambda\| \bm{a}_s + \sum_{j=1}^N c_j \bm{x}_j\|_2^2   \\
    &= \sum_{i = 1}^N ( \sum_{j=1}^N c_j \langle \bm{x}_j, \bm{x}_i\rangle - y_i)^2 + \lambda\sum_{j_1=1}^N \sum_{j_2=1}^N c_{j_1}c_{j_2} \langle \bm{x}_{j_1}\bm{x}_{j_2}\rangle + \lambda \|\bm{a}_s\|_2^2
\end{align*}
Introduce \(\bm{K} = [\langle \bm{x}_i, \bm{x}_j \rangle]_{i, j = 1}^N \in \R^{N \times N}\).
\begin{align*}
    &\sum_{i = 1}^N ( \sum_{j=1}^N c_j \langle \bm{x}_j, \bm{x}_i\rangle - y_i)^2 + \lambda\sum_{j_1=1}^N \sum_{j_2=1}^N c_{j_1}c_{j_2} \langle \bm{x}_{j_1}\bm{x}_{j_2}\rangle + \lambda \|\bm{a}_s\|_2^2 \\
    &=\sum_{i = 1}^N ((\bm{K}^T\bm{c})_i - y_i)^2 + \lambda \bm{c}^T \bm{K} \bm{c} + \lambda \|\bm{a}_s\|_2^2  
\end{align*}
We know:
\begin{align*}
    &\text{minimize}_{\bm{a} \in \R^n} \sum_{i = 1}^N ((\bm{K}^T\bm{c})_i - y_i)^2 + \lambda \bm{c}^T \bm{K} \bm{c} + \lambda \|\bm{a}_s\|^2   \\
    &\iff \text{minimize}_{\bm{c} \in \R^n} \sum_{i = 1}^N ((\bm{K}^T\bm{c})_i - y_i)^2 + \lambda \bm{c}^T \bm{K} \bm{c} \\ 
    &\text{ and } \text{minimize}_{\bm{a}_s \in \R^n, \langle \bm{a}_s, \bm{x}_i\rangle = 0} \lambda \|\bm{a}_s\|^2  \\
    &\iff \bm{a}_s^* = \bm{0} \\
    &\text{ and } \bm{c}^* = \text{argmin}_{\bm{c} \in \R^n} \sum_{i = 1}^N ((\bm{K}^T\bm{c})_i - y_i)^2 + \lambda \bm{c}^T \bm{K} \bm{c} \\ 
\end{align*}
In conclusion, the optimal \(\bm{a}^* = \sum_{i = 1}^N c_i^* \bm{x}_i\)

(b)


\section*{Question 4:}
4. Let \(f(\bm{x}) = \bm{x}^T\bm{A}\bm{x} + 2\bm{b}^T\bm{x} + c\), where \(\bm{A} \in \R^{n \times n}\) is a symmetric positive semidefinite matrix, \(b \in \R^n\), and \(c \in \R\). \\
(a) Prove that \(\bm{x}\) is a global minimizer of \(f\) if and only if \(\bm{A}\bm{x} = -\bm{b}\). \\
(b) Prove that \(f\) is bounded below over \(\R^n\) if and only if \(\bm{b} \in \{\bm{A}\bm{y}: \bm{y} \in \R^n\}\). \\
\subsection*{Answer :}


\section*{Question 5:}
5. We consider the following optimization problem:
\begin{align}
    \text{minimize}_{\bm{x} \in \R^n} f(\bm{x}) = log\left( \sum_{i = 1}^m \text{exp}(\bm{a}_i^T \bm{x} + b_i)\right)
\end{align}

where \(\bm{a}_1, \cdots \bm{a}_m \in \R^n \text{ and } b_1, \cdots b_m \in \R\) are given. \\

(a) Find the gradient of \(f(\bm{x})\). \\
(b) If we use gradient descent to solve Problem (1), will it converge to the global minimizer? Please justify your answer.


\subsection*{Answer}
\end{document}